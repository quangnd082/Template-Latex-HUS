\chapter*{Code}
\addcontentsline{toc}{chapter}{Code}

\label{Mã nguồn hệ thống}

***Một số code quan trọng trong hệ thống, các code còn lại đẩy trên google drive: \href{https://drive.google.com/drive/folders/1pspIib_DGpTEYfZ0l5Xm07Y1BCCl97d_?fbclid=IwAR1OaNPHzwGAtbG9NQQnDHE2cXxzpvXHW-w8ted6p9JCAnQck757pyKbOlw}{Source Code System} 

\section{Emotion Recognition}

\begin{lstlisting}
	import glob
	import shutil
	import cv2 
	from PIL import Image
	import PIL
	import os
	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	import seaborn as sns
	import random 
	from sklearn.model_selection import train_test_split
	
	import warnings
	# Ignore waring
	warnings.filterwarnings('ignore')
	
	from google.colab import drive
	drive.mount('/content/drive')
	
	emotion_path = ['/angry/*', '/fear/*', '/happy/*', '/sad/*', '/surprise/*', '/neutral/*']
	Class_name = ['angry', 'fear', 'happy', 'sad', 'surprise', 'neutral']
	
	train_path = '/content/drive/MyDrive/Dataset/emotions/train'
	
	train_count=[]
	for i in Class_name:
	train_count.append(len(os.listdir("/content/drive/MyDrive/Dataset/emotions/train/"+i+"/")))
	plt.bar(Class_name,train_count)
	
	X = []
	y = []
	for i, path in enumerate(emotion_path):
	for name in glob.glob(train_path+path):
	img = cv2.imread(name)
	img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
	img = cv2.resize(img, (48,48))
	X.append((img))
	y.append(i)
	len(X)
	
	(X_train,X_test,y_train,y_test) = train_test_split(X,y,test_size=0.2,random_state=42)
	(X_train,X_val,y_train,y_val) = train_test_split(X_train,y_train,test_size=0.1,random_state=42)
	
	X_train = X_train /255
	X_val = X_val /255
	X_test = X_test / 255
	
	from sklearn.preprocessing import OneHotEncoder
	encoder = OneHotEncoder()
	encoder.fit(y_train.reshape(-1,1))
	y_train = encoder.transform(y_train.reshape(-1,1)).toarray()
	y_val = encoder.transform(y_val.reshape(-1,1)).toarray()
	y_test = encoder.transform(y_test.reshape(-1,1)).toarray()
	
	import numpy as np
	from matplotlib import pyplot as plt
	import tensorflow as tf
	import keras
	from keras.preprocessing import image
	from keras.models import Sequential
	from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization
	from keras import regularizers
	
	import cv2
	model= tf.keras.models.Sequential()
	model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))
	model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))
	model.add(BatchNormalization())
	model.add(MaxPool2D(pool_size=(2, 2)))
	model.add(Dropout(0.25))
	
	model.add(Conv2D(128,(5,5), padding='same', activation='relu'))
	model.add(BatchNormalization())
	model.add(MaxPool2D(pool_size=(2, 2)))
	model.add(Dropout(0.25))
	
	model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))
	model.add(BatchNormalization())
	model.add(MaxPool2D(pool_size=(2, 2)))
	model.add(Dropout(0.25))
	
	model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))
	model.add(BatchNormalization())
	model.add(MaxPool2D(pool_size=(2, 2)))
	model.add(Dropout(0.25))
	
	model.add(Flatten()) 
	model.add(Dense(256,activation = 'relu'))
	model.add(BatchNormalization())
	model.add(Dropout(0.25))
	
	model.add(Dense(512,activation = 'relu'))
	model.add(BatchNormalization())
	model.add(Dropout(0.25))
	
	model.add(Dense(6, activation='softmax'))
	
	from keras.utils.vis_utils import plot_model 
	plot_model(model,show_shapes=True,show_layer_names=True)
	
	learning_rate = 0.0001  # Toc do hoc ban dau
	
	# Khoi tao learning rate scheduler
	optimizer1 = tf.keras.optimizers.Adam(learning_rate = 0.0001)
	
	model.compile(optimizer = optimizer1, loss = 'categorical_crossentropy',metrics =['accuracy'])
	
	history = model.fit(X_train, y_train,
	epochs=40,validation_data=(X_val, y_val),batch_size = 64,shuffle = True)
	
	h = history
	
	# plot the loss value
	plt.plot(h.history['loss'], label='train loss')
	plt.plot(h.history['val_loss'], label='validation loss')
	plt.legend()
	plt.show()
	
	#plot the accuracy value
	plt.plot(h.history['accuracy'], label='train accuracy')
	plt.plot(h.history['val_accuracy'], label='validation accuracy')
	plt.legend()
	plt.show()
	
	model.save('model_emotions_final_67.h5')
	
	from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
	y_hat = model1.predict(X_test)
	y_test1 =np.argmax(y_test,axis =1)
	y_hat1 = np.argmax(y_hat,axis =1)
	print(classification_report(y_test1, y_hat1))
	
	import seaborn as sns
	
	cm = confusion_matrix(y_test1, y_hat1)
	print(cm)
	sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
	
	# Cau hinh truc x va truc y 
	tick_labels = ['angry', 'fear', 'happy', 'sad', 'surprise', 'neutral']
	plt.xticks(np.arange(len(tick_labels)) + 0.5, tick_labels)
	plt.yticks(np.arange(len(tick_labels)) + 0.5, tick_labels)
	
	#Dat ten cac truc 
	plt.xlabel('Predicted')
	plt.ylabel('True')
	
	# Hien thi bieu do
	plt.title('Confusion Matrix')
	plt.show()
\end{lstlisting}

\section{Mask Detection}

\begin{lstlisting}
	import matplotlib.image as mpimg
	import glob
	import shutil
	import cv2 
	from PIL import Image
	import PIL
	import os
	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.metrics import confusion_matrix
	import seaborn as sns
	from sklearn.model_selection import train_test_split
	import tensorflow
	from tensorflow.keras.models import Model
	from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Reshape
	from tensorflow.keras.layers import Conv2D, MaxPooling2D
	
	import warnings
	# Ignore waring
	warnings.filterwarnings('ignore')
	
	Class_name = ['without_mask', 'with_mask', 'mask_weared_incorrect']
	train_count=[]
	for i in Class_name:
	train_count.append(len(os.listdir("dataset/"+i+"/")))
	plt.bar(Class_name,train_count)
	
	mask_path = ['/without_mask/*','/with_mask/*','/mask_weared_incorrect/*' ]
	train_path = 'dataset'
	
	X = []
	y = []
	for i, path in enumerate(mask_path):
	for name in glob.glob(train_path + path):
	img = cv2.imread(name)
	img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
	img = cv2.resize(img,(48,48))
	X.append((img))
	y.append(i)
	len(X)
	
	# np.array
	X = np.array(X)/255.
	y = np.array(y)
	
	X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=2)
	
	from sklearn.preprocessing import OneHotEncoder
	encoder = OneHotEncoder()
	encoder.fit(y_train.reshape(-1,1))
	y_train = encoder.transform(y_train.reshape(-1,1)).toarray()
	y_test = encoder.transform(y_test.reshape(-1,1)).toarray()
	print(y_train)
	
	#CNN
	inp = Input(shape = (48,48,3))
	cnn = Conv2D(filters =16,kernel_size = 3,activation ='relu')(inp)
	pooling = MaxPooling2D(pool_size =(2,2))(cnn)
	drop = Dropout(0.2)(pooling)
	
	cnn = Conv2D(filters =32,kernel_size = 3,activation ='relu')(inp)
	pooling = MaxPooling2D(pool_size =(2,2))(cnn)
	drop = Dropout(0.2)(pooling)
	
	cnn = Conv2D(filters =64,kernel_size = 4,activation ='relu')(inp)
	pooling = MaxPooling2D(pool_size =(2,2))(cnn)
	drop = Dropout(0.2)(pooling)
	
	cnn = Conv2D(filters =128, kernel_size =4,activation ='relu')(drop)
	pooling = MaxPooling2D(pool_size =(2,2))(cnn)
	drop = Dropout(0.2)(pooling)
	
	f =Flatten()(pooling)
	
	fc1 = Dense(units =128, activation ='relu')(f)
	fc2 = Dense(units =64, activation ='relu')(fc1)
	fc3 = Dense(units =32, activation ='relu')(fc2)
	out = Dense(units =3, activation ='softmax')(fc3)
	
	model =Model(inputs = inp,outputs =out)
	model.summary()
	
	optimizer1 = tensorflow.keras.optimizers.Adam(learning_rate = 0.0001)
	model.compile(optimizer = optimizer1, loss = 'categorical_crossentropy',metrics =['accuracy'])
	
	history = model.fit(X_train,y_train,batch_size =16, epochs =25,validation_data =(X_test,y_test))
	
	from matplotlib import pyplot  as plt
	plt.plot(history.history['accuracy'])
	plt.plot(history.history['val_accuracy'])
	plt.title('model accuracy')
	plt.ylabel('accuracy')
	plt.xlabel('epoch')
	plt.legend(['train','test'],loc ='upper left')
	plt.show()
	#summarize history for loss
	plt.plot(history.history['loss'])
	plt.plot(history.history['val_loss'])
	plt.title('model loss')
	plt.ylabel('loss')
	plt.xlabel('epoch')
	plt.legend(['train','test'],loc ='upper left')
	plt.show()
	
	model.save('model2.h5')
	
	from sklearn.metrics import classification_report
	y_hat = model1.predict(X_test)
	y_test1 =np.argmax(y_test,axis =1)
	y_hat1 = np.argmax(y_hat,axis =1)
	print(classification_report(y_test1, y_hat1))
	
	matrix = confusion_matrix(y_test1,y_hat1)
	print(matrix)
	sns.heatmap(matrix, annot=True, fmt='d')
	
	tick_labels = ['Nomask', 'Mask','Incorectmask']
	plt.xticks(np.arange(len(tick_labels)) + 0.5, tick_labels)
	plt.yticks(np.arange(len(tick_labels)) + 0.5, tick_labels)
	
	#Dat ten cho cac truc
	plt.xlabel('Predicted')
	plt.ylabel('True')
	
	#Hien thi bieu do
	plt.title('Confusion Matrix')
	plt.show()
\end{lstlisting}

